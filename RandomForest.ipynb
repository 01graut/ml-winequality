{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import MinMaxScaler, StandardScaler\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# import numpy\n",
    "import numpy as np\n",
    "\n",
    "# import seaborn\n",
    "import seaborn as sb\n",
    "\n",
    "# import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"TrainTestWorkflow\").getOrCreate()\n",
    "\n",
    "# Load the datasets\n",
    "train_data_path = \"/workspaces/ml-winequality/dataset/TrainingDataset.csv\"\n",
    "test_data_path = \"/workspaces/ml-winequality/dataset/ValidationDataset.csv\"\n",
    "\n",
    "train_df = spark.read.csv(train_data_path,header=True, \n",
    "                      inferSchema=True,\n",
    "                      sep=';'\n",
    "                      ,quote='\"')\n",
    "test_df = spark.read.csv(test_data_path,header=True, \n",
    "                      inferSchema=True,\n",
    "                      sep=';'\n",
    "                      ,quote='\"')\n",
    "\n",
    "# Used copilot how to get rid of quotes from colum header\n",
    "new_column_names = [col_name.strip('\"') for col_name in train_df.columns]\n",
    "train_df = train_df.toDF(*new_column_names)\n",
    "# train_df=train_df.drop(\"total sulfur dioxide\")\n",
    "# print(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+-------------------+------------------+--------------------+-------------------+--------------------+--------------------+-------------------+------------------+------------------+------------------+\n",
      "|summary|    fixed acidity|   volatile acidity|        citric acid|    residual sugar|           chlorides|free sulfur dioxide|total sulfur dioxide|             density|                 pH|         sulphates|           alcohol|           quality|\n",
      "+-------+-----------------+-------------------+-------------------+------------------+--------------------+-------------------+--------------------+--------------------+-------------------+------------------+------------------+------------------+\n",
      "|  count|             1279|               1279|               1279|              1279|                1279|               1279|                1279|                1279|               1279|              1279|              1279|              1279|\n",
      "|   mean|8.136669272869451| 0.5304026583268179|0.25653635652853685|  2.50512118842846| 0.08653322908522246| 16.082877247849883|   46.00312744331509|  0.9965030023455833| 3.3223377638780343|0.6501954652071926|10.468139171227493|5.6411258795934325|\n",
      "| stddev| 1.62643705835024|0.18019761285860228|0.19234683571822775|1.4281867850973125|0.047000052756637024| 10.548020270224658|   32.45911702682881|0.001814834341261...|0.14877883220527952|0.1551395230836167| 1.059036248732797|0.8098942041137892|\n",
      "|    min|              4.6|               0.12|                0.0|               0.9|               0.034|                1.0|                 6.0|             0.99007|               2.74|              0.33|               8.5|                 3|\n",
      "|    max|             15.6|               1.58|                1.0|              15.4|               0.611|               72.0|               289.0|             1.00369|               4.01|               2.0|              14.0|                 8|\n",
      "+-------+-----------------+-------------------+-------------------+------------------+--------------------+-------------------+--------------------+--------------------+-------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DataFrame.corr() missing 2 required positional arguments: 'col1' and 'col2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns\n",
      "\u001b[0;31mTypeError\u001b[0m: DataFrame.corr() missing 2 required positional arguments: 'col1' and 'col2'"
     ]
    }
   ],
   "source": [
    "train_df.corr().columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got float64.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(corr_matrix\u001b[38;5;241m.\u001b[39mcolumns)):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(a):\n\u001b[0;32m---> 18\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mabs\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcorr_matrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.7\u001b[39m:\n\u001b[1;32m     19\u001b[0m             name \u001b[38;5;241m=\u001b[39m corr_matrix\u001b[38;5;241m.\u001b[39mcolumns[a]\n\u001b[1;32m     20\u001b[0m             features_to_print\u001b[38;5;241m.\u001b[39madd(name)\n",
      "File \u001b[0;32m/workspaces/ml-winequality/.venv/lib/python3.10/site-packages/pyspark/sql/utils.py:174\u001b[0m, in \u001b[0;36mtry_remote_functions.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(functions, f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/ml-winequality/.venv/lib/python3.10/site-packages/pyspark/sql/functions.py:615\u001b[0m, in \u001b[0;36mabs\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;129m@try_remote_functions\u001b[39m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mabs\u001b[39m(col: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Column:\n\u001b[1;32m    587\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;124;03m    Computes the absolute value.\u001b[39;00m\n\u001b[1;32m    589\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;124;03m    +-------+\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 615\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_invoke_function_over_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mabs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/ml-winequality/.venv/lib/python3.10/site-packages/pyspark/sql/functions.py:105\u001b[0m, in \u001b[0;36m_invoke_function_over_columns\u001b[0;34m(name, *cols)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_invoke_function_over_columns\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Column:\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    Invokes n-ary JVM function identified by name\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    and wraps the result with :class:`~pyspark.sql.Column`.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _invoke_function(name, \u001b[38;5;241m*\u001b[39m(_to_java_column(col) \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols))\n",
      "File \u001b[0;32m/workspaces/ml-winequality/.venv/lib/python3.10/site-packages/pyspark/sql/functions.py:105\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_invoke_function_over_columns\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Column:\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    Invokes n-ary JVM function identified by name\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    and wraps the result with :class:`~pyspark.sql.Column`.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _invoke_function(name, \u001b[38;5;241m*\u001b[39m(\u001b[43m_to_java_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols))\n",
      "File \u001b[0;32m/workspaces/ml-winequality/.venv/lib/python3.10/site-packages/pyspark/sql/column.py:65\u001b[0m, in \u001b[0;36m_to_java_column\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     63\u001b[0m     jcol \u001b[38;5;241m=\u001b[39m _create_column_from_name(col)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m     66\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN_OR_STR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     67\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m     68\u001b[0m     )\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m jcol\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got float64."
     ]
    }
   ],
   "source": [
    "# for a in range(len(train_df.corr().columns)):\n",
    "#     for b in range(a):\n",
    "#         if abs(train_df.corr().iloc[a,b]) >0.7:\n",
    "#             name = train_df.corr().columns[a]\n",
    "#             print(name)\n",
    "\n",
    "df=train_df.toPandas()\n",
    "\n",
    "# Calculate the correlation matrix once\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "# Store names of features to print to avoid duplicates\n",
    "features_to_print = set()\n",
    "\n",
    "# Iterate through the lower triangle of the correlation matrix\n",
    "for a in range(len(corr_matrix.columns)):\n",
    "    for b in range(a):\n",
    "        if abs(corr_matrix.iloc[a, b]) > 0.7:\n",
    "            name = corr_matrix.columns[a]\n",
    "            features_to_print.add(name)\n",
    "\n",
    "# Print each feature name only once\n",
    "for name in features_to_print:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/06 20:55:29 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/04/06 20:55:29 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "[Stage 28:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|       fixed acidity|    volatile acidity|         citric acid|      residual sugar|           chlorides| free sulfur dioxide|total sulfur dioxide|             density|                  pH|           sulphates|             alcohol|             quality|            col_name|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                 1.0| -0.2507817264410709|  0.6611058850504964| 0.06355830685756873| 0.10597708845864431| -0.1515799776127361|-0.10244909556003681|  0.6345821815819864| -0.6631587796097901| 0.19552098272064433| -0.1167125034865953|  0.1248228593705539|       fixed acidity|\n",
      "| -0.2507817264410709|                 1.0| -0.5548027427413004|0.014570846117229856| 0.07274784771522523|-0.01305412865950...| 0.08923680370014014| 0.04754634489152018|  0.2153947415863092|-0.26054878876475146|-0.22337018017816562| -0.3927626309668435|    volatile acidity|\n",
      "|  0.6611058850504964| -0.5548027427413004|                 1.0| 0.12056565931394854|  0.2063001247117902|-0.07068625656293769| 0.03839841287297307|  0.3326414239082598| -0.5218220897417095| 0.32320313403967654|  0.0948669533411843| 0.21764383951364966|         citric acid|\n",
      "| 0.06355830685756873|0.014570846117229856| 0.12056565931394854|                 1.0| 0.07647449133079466|  0.2183825569192325| 0.22609761041283244| 0.35683338651336166|-0.06025977158138758|0.004016114252282147|-0.00161735579366...|0.011534430598313383|      residual sugar|\n",
      "| 0.10597708845864431| 0.07274784771522523|  0.2063001247117902| 0.07647449133079466|                 1.0|0.009364240568896929| 0.05486502168929638| 0.20798495847862647|-0.28071590554296916|   0.409396054504415|-0.21722300686600574|-0.12432889948915009|           chlorides|\n",
      "| -0.1515799776127361|-0.01305412865950...|-0.07068625656293769|  0.2183825569192325|0.009364240568896929|                 1.0|  0.6440307296815603|0.005017108782829598| 0.06201515167895541| 0.04345269574688227|-0.08107125063579597|-0.04780866045840105| free sulfur dioxide|\n",
      "|-0.10244909556003681| 0.08923680370014014| 0.03839841287297307| 0.22609761041283244| 0.05486502168929638|  0.6440307296815603|                 1.0|  0.1048526143326447|-0.08187773554274619|-0.00794031145004...|-0.23340163134051256| -0.1928337027771241|total sulfur dioxide|\n",
      "|  0.6345821815819864| 0.04754634489152018|  0.3326414239082598| 0.35683338651336166| 0.20798495847862647|0.005017108782829598|  0.1048526143326447|                 1.0| -0.2974673409201778| 0.15176788754028464|  -0.559176330712838|-0.18768387214831447|             density|\n",
      "| -0.6631587796097901|  0.2153947415863092| -0.5218220897417095|-0.06025977158138758|-0.28071590554296916| 0.06201515167895541|-0.08187773554274619| -0.2974673409201778|                 1.0| -0.1625386021035273| 0.23541689972369978|-0.05829473177002926|                  pH|\n",
      "| 0.19552098272064433|-0.26054878876475146| 0.32320313403967654|0.004016114252282147|   0.409396054504415| 0.04345269574688227|-0.00794031145004...| 0.15176788754028464| -0.1625386021035273|                 1.0|    0.10588233010649| 0.26740997071008843|           sulphates|\n",
      "| -0.1167125034865953|-0.22337018017816562|  0.0948669533411843|-0.00161735579366...|-0.21722300686600574|-0.08107125063579597|-0.23340163134051256|  -0.559176330712838| 0.23541689972369978|    0.10588233010649|                 1.0|  0.4757948892163372|             alcohol|\n",
      "|  0.1248228593705539| -0.3927626309668435| 0.21764383951364966|0.011534430598313383|-0.12432889948915009|-0.04780866045840105| -0.1928337027771241|-0.18768387214831447|-0.05829473177002926| 0.26740997071008843|  0.4757948892163372|                 1.0|             quality|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "df=train_df\n",
    "# Assemble features into a vector\n",
    "assembler = VectorAssembler(inputCols=df.columns, outputCol=\"features\")\n",
    "vector_df = assembler.transform(df)\n",
    "\n",
    "# Calculate Correlation Matrix\n",
    "correlation_matrix = Correlation.corr(vector_df, \"features\").head()\n",
    "\n",
    "# Convert correlation matrix to a DataFrame for easier processing\n",
    "corr_df = correlation_matrix[0].toArray().tolist()\n",
    "corr_df = spark.createDataFrame(corr_df)\n",
    "\n",
    "# Add column names to the correlation DataFrame\n",
    "for i, col in enumerate(df.columns):\n",
    "    corr_df = corr_df.withColumnRenamed(f\"_{i+1}\", col)\n",
    "\n",
    "# Add row index as a column to join with column names\n",
    "corr_df = corr_df.withColumn(\"row_index\", monotonically_increasing_id())\n",
    "column_names_df = spark.createDataFrame([(name,) for name in df.columns], [\"col_name\"]).withColumn(\"row_index\", monotonically_increasing_id())\n",
    "\n",
    "# Join column names\n",
    "corr_df = corr_df.join(column_names_df, \"row_index\").drop(\"row_index\")\n",
    "\n",
    "corr_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Delete rows with missing values\n",
    "df = train_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used copilot how to get rid of quotes from colum header\n",
    "new_column_names = [col_name.strip('\"') for col_name in train_df.columns]\n",
    "train_df = train_df.toDF(*new_column_names)\n",
    "\n",
    "# Used copilot how to get rid of quotes from colum header\n",
    "new_column_names = [col_name.strip('\"') for col_name in test_df.columns]\n",
    "test_df = test_df.toDF(*new_column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imbalance data set, Resize using SMOTE\n",
    "# import SMOTE\n",
    "# sm = SMOTE(random_state=14)\n",
    "# # X_train, Y_train = sm.fit_resample(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "\n",
    "# Assemble features\n",
    "featureCols = train_df.columns[:-1]  # Assuming the last column is the label\n",
    "assembler = VectorAssembler(inputCols=featureCols, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Initialize the model\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"quality\")\n",
    "\n",
    "# Create a Pipeline\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "fitted_pipeline = pipeline.fit(train_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model F1 Score on Test Data: 0.4865269070010449\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import *\n",
    "# Make predictions on test data\n",
    "# Make predictions\n",
    "predictions = fitted_pipeline.transform(test_df)\n",
    "\n",
    "predictions = predictions.withColumn(\"prediction\", round(col(\"prediction\"),0).cast(\"double\"))\n",
    "# Evaluate the best model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"quality\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_score = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Best Model F1 Score on Test Data: {f1_score}\")\n",
    "# Best Model F1 Score on Test Data: 0.4865269070010449\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################# Part 2\n",
    "Using MinMaxScalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/30 18:06:09 WARN DAGScheduler: Broadcasting large task binary with size 1133.9 KiB\n",
      "24/03/30 18:06:09 WARN DAGScheduler: Broadcasting large task binary with size 1347.6 KiB\n",
      "24/03/30 18:06:09 WARN DAGScheduler: Broadcasting large task binary with size 1528.8 KiB\n",
      "24/03/30 18:06:10 WARN DAGScheduler: Broadcasting large task binary with size 1670.7 KiB\n",
      "24/03/30 18:06:10 WARN DAGScheduler: Broadcasting large task binary with size 1777.1 KiB\n",
      "24/03/30 18:06:10 WARN DAGScheduler: Broadcasting large task binary with size 1851.3 KiB\n",
      "24/03/30 18:06:10 WARN DAGScheduler: Broadcasting large task binary with size 1753.6 KiB\n",
      "24/03/30 18:06:10 WARN DAGScheduler: Broadcasting large task binary with size 1404.9 KiB\n",
      "24/03/30 18:06:10 WARN DAGScheduler: Broadcasting large task binary with size 1194.3 KiB\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Assemble features\n",
    "featureCols = train_df.columns[:-1]  # Assuming the last column is the label\n",
    "\n",
    "# MINMAXSCALAR\n",
    "\n",
    "# Assemble features\n",
    "assembler = VectorAssembler(inputCols=featureCols, outputCol=\"assembledFeatures\")\n",
    "\n",
    "# Normalize features using Min-Max Scaling\n",
    "scaler = MinMaxScaler(inputCol=\"assembledFeatures\", outputCol=\"normalizedFeatures\")\n",
    "\n",
    "# Initialize the model\n",
    "model = RandomForestRegressor(featuresCol=\"normalizedFeatures\", labelCol='quality',\n",
    "                              maxDepth=20,\n",
    "                              numTrees=25,\n",
    "                              seed=42,\n",
    "                              )\n",
    "\n",
    "\n",
    "# Create a Pipeline\n",
    "pipeline2 = Pipeline(stages=[assembler, scaler, model])\n",
    "\n",
    "# Train the model\n",
    "fitted_pipeline2 = pipeline2.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model F1 Score on Test Data: 0.6122891944715902\n"
     ]
    }
   ],
   "source": [
    "# Evaluate using MixMax Scalar\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import *\n",
    "# Make predictions on test data\n",
    "# Make predictions\n",
    "predictions = fitted_pipeline2.transform(test_df)\n",
    "\n",
    "predictions = predictions.withColumn(\"prediction\", round(col(\"prediction\"),0).cast(\"double\"))\n",
    "# Evaluate the best model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"quality\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_score = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Best Model F1 Score on Test Data: {f1_score}\")\n",
    "# Best Model F1 Score on Test Data: 0.6034063260340633"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part3 Using Cross Validator + MinMax Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/30 17:45:00 WARN DAGScheduler: Broadcasting large task binary with size 1076.3 KiB\n",
      "24/03/30 17:45:00 WARN DAGScheduler: Broadcasting large task binary with size 1205.9 KiB\n",
      "24/03/30 17:45:00 WARN DAGScheduler: Broadcasting large task binary with size 1302.2 KiB\n",
      "24/03/30 17:45:00 WARN DAGScheduler: Broadcasting large task binary with size 1370.4 KiB\n",
      "24/03/30 17:45:00 WARN DAGScheduler: Broadcasting large task binary with size 1419.2 KiB\n",
      "24/03/30 17:45:02 WARN DAGScheduler: Broadcasting large task binary with size 1045.8 KiB\n",
      "24/03/30 17:45:02 WARN DAGScheduler: Broadcasting large task binary with size 1294.7 KiB\n",
      "24/03/30 17:45:03 WARN DAGScheduler: Broadcasting large task binary with size 1045.8 KiB\n",
      "24/03/30 17:45:03 WARN DAGScheduler: Broadcasting large task binary with size 1294.7 KiB\n",
      "24/03/30 17:45:03 WARN DAGScheduler: Broadcasting large task binary with size 1524.4 KiB\n",
      "24/03/30 17:45:03 WARN DAGScheduler: Broadcasting large task binary with size 1724.7 KiB\n",
      "24/03/30 17:45:04 WARN DAGScheduler: Broadcasting large task binary with size 1884.5 KiB\n",
      "24/03/30 17:45:04 WARN DAGScheduler: Broadcasting large task binary with size 2004.5 KiB\n",
      "24/03/30 17:45:04 WARN DAGScheduler: Broadcasting large task binary with size 2016.1 KiB\n",
      "24/03/30 17:45:10 WARN DAGScheduler: Broadcasting large task binary with size 1099.2 KiB\n",
      "24/03/30 17:45:10 WARN DAGScheduler: Broadcasting large task binary with size 1231.9 KiB\n",
      "24/03/30 17:45:10 WARN DAGScheduler: Broadcasting large task binary with size 1330.7 KiB\n",
      "24/03/30 17:45:10 WARN DAGScheduler: Broadcasting large task binary with size 1401.9 KiB\n",
      "24/03/30 17:45:10 WARN DAGScheduler: Broadcasting large task binary with size 1452.7 KiB\n",
      "24/03/30 17:45:12 WARN DAGScheduler: Broadcasting large task binary with size 1074.7 KiB\n",
      "24/03/30 17:45:12 WARN DAGScheduler: Broadcasting large task binary with size 1336.6 KiB\n",
      "24/03/30 17:45:13 WARN DAGScheduler: Broadcasting large task binary with size 1074.7 KiB\n",
      "24/03/30 17:45:13 WARN DAGScheduler: Broadcasting large task binary with size 1336.6 KiB\n",
      "24/03/30 17:45:13 WARN DAGScheduler: Broadcasting large task binary with size 1571.8 KiB\n",
      "24/03/30 17:45:14 WARN DAGScheduler: Broadcasting large task binary with size 1766.8 KiB\n",
      "24/03/30 17:45:14 WARN DAGScheduler: Broadcasting large task binary with size 1913.5 KiB\n",
      "24/03/30 17:45:14 WARN DAGScheduler: Broadcasting large task binary with size 2023.1 KiB\n",
      "24/03/30 17:45:14 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/03/30 17:45:19 WARN DAGScheduler: Broadcasting large task binary with size 1068.9 KiB\n",
      "24/03/30 17:45:20 WARN DAGScheduler: Broadcasting large task binary with size 1202.9 KiB\n",
      "24/03/30 17:45:20 WARN DAGScheduler: Broadcasting large task binary with size 1304.8 KiB\n",
      "24/03/30 17:45:20 WARN DAGScheduler: Broadcasting large task binary with size 1379.4 KiB\n",
      "24/03/30 17:45:20 WARN DAGScheduler: Broadcasting large task binary with size 1433.5 KiB\n",
      "24/03/30 17:45:22 WARN DAGScheduler: Broadcasting large task binary with size 1093.5 KiB\n",
      "24/03/30 17:45:22 WARN DAGScheduler: Broadcasting large task binary with size 1361.4 KiB\n",
      "24/03/30 17:45:23 WARN DAGScheduler: Broadcasting large task binary with size 1093.5 KiB\n",
      "24/03/30 17:45:23 WARN DAGScheduler: Broadcasting large task binary with size 1361.4 KiB\n",
      "24/03/30 17:45:23 WARN DAGScheduler: Broadcasting large task binary with size 1602.1 KiB\n",
      "24/03/30 17:45:24 WARN DAGScheduler: Broadcasting large task binary with size 1805.0 KiB\n",
      "24/03/30 17:45:24 WARN DAGScheduler: Broadcasting large task binary with size 1958.3 KiB\n",
      "24/03/30 17:45:24 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/03/30 17:45:24 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/03/30 17:45:30 WARN DAGScheduler: Broadcasting large task binary with size 1117.1 KiB\n",
      "24/03/30 17:45:30 WARN DAGScheduler: Broadcasting large task binary with size 1254.0 KiB\n",
      "24/03/30 17:45:30 WARN DAGScheduler: Broadcasting large task binary with size 1354.2 KiB\n",
      "24/03/30 17:45:30 WARN DAGScheduler: Broadcasting large task binary with size 1423.6 KiB\n",
      "24/03/30 17:45:30 WARN DAGScheduler: Broadcasting large task binary with size 1472.9 KiB\n",
      "24/03/30 17:45:33 WARN DAGScheduler: Broadcasting large task binary with size 1082.6 KiB\n",
      "24/03/30 17:45:33 WARN DAGScheduler: Broadcasting large task binary with size 1354.8 KiB\n",
      "24/03/30 17:45:34 WARN DAGScheduler: Broadcasting large task binary with size 1082.6 KiB\n",
      "24/03/30 17:45:35 WARN DAGScheduler: Broadcasting large task binary with size 1354.8 KiB\n",
      "24/03/30 17:45:35 WARN DAGScheduler: Broadcasting large task binary with size 1602.3 KiB\n",
      "24/03/30 17:45:35 WARN DAGScheduler: Broadcasting large task binary with size 1808.0 KiB\n",
      "24/03/30 17:45:35 WARN DAGScheduler: Broadcasting large task binary with size 1957.7 KiB\n",
      "24/03/30 17:45:35 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/03/30 17:45:35 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/03/30 17:45:41 WARN DAGScheduler: Broadcasting large task binary with size 1059.3 KiB\n",
      "24/03/30 17:45:41 WARN DAGScheduler: Broadcasting large task binary with size 1195.1 KiB\n",
      "24/03/30 17:45:41 WARN DAGScheduler: Broadcasting large task binary with size 1300.2 KiB\n",
      "24/03/30 17:45:41 WARN DAGScheduler: Broadcasting large task binary with size 1374.2 KiB\n",
      "24/03/30 17:45:41 WARN DAGScheduler: Broadcasting large task binary with size 1427.7 KiB\n",
      "24/03/30 17:45:43 WARN DAGScheduler: Broadcasting large task binary with size 1072.4 KiB\n",
      "24/03/30 17:45:43 WARN DAGScheduler: Broadcasting large task binary with size 1342.2 KiB\n",
      "24/03/30 17:45:44 WARN DAGScheduler: Broadcasting large task binary with size 1072.3 KiB\n",
      "24/03/30 17:45:44 WARN DAGScheduler: Broadcasting large task binary with size 1342.2 KiB\n",
      "24/03/30 17:45:45 WARN DAGScheduler: Broadcasting large task binary with size 1576.7 KiB\n",
      "24/03/30 17:45:45 WARN DAGScheduler: Broadcasting large task binary with size 1773.6 KiB\n",
      "24/03/30 17:45:45 WARN DAGScheduler: Broadcasting large task binary with size 1926.5 KiB\n",
      "24/03/30 17:45:45 WARN DAGScheduler: Broadcasting large task binary with size 2037.6 KiB\n",
      "24/03/30 17:45:45 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/03/30 17:45:51 WARN DAGScheduler: Broadcasting large task binary with size 1066.1 KiB\n",
      "24/03/30 17:45:51 WARN DAGScheduler: Broadcasting large task binary with size 1193.1 KiB\n",
      "24/03/30 17:45:51 WARN DAGScheduler: Broadcasting large task binary with size 1297.0 KiB\n",
      "24/03/30 17:45:51 WARN DAGScheduler: Broadcasting large task binary with size 1376.2 KiB\n",
      "24/03/30 17:45:51 WARN DAGScheduler: Broadcasting large task binary with size 1430.3 KiB\n",
      "24/03/30 17:45:53 WARN DAGScheduler: Broadcasting large task binary with size 1082.2 KiB\n",
      "24/03/30 17:45:54 WARN DAGScheduler: Broadcasting large task binary with size 1354.7 KiB\n",
      "24/03/30 17:45:55 WARN DAGScheduler: Broadcasting large task binary with size 1082.2 KiB\n",
      "24/03/30 17:45:55 WARN DAGScheduler: Broadcasting large task binary with size 1354.7 KiB\n",
      "24/03/30 17:45:55 WARN DAGScheduler: Broadcasting large task binary with size 1590.3 KiB\n",
      "24/03/30 17:45:55 WARN DAGScheduler: Broadcasting large task binary with size 1786.6 KiB\n",
      "24/03/30 17:45:56 WARN DAGScheduler: Broadcasting large task binary with size 1931.5 KiB\n",
      "24/03/30 17:45:56 WARN DAGScheduler: Broadcasting large task binary with size 2035.9 KiB\n",
      "24/03/30 17:45:56 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/03/30 17:46:02 WARN DAGScheduler: Broadcasting large task binary with size 1073.9 KiB\n",
      "24/03/30 17:46:02 WARN DAGScheduler: Broadcasting large task binary with size 1202.8 KiB\n",
      "24/03/30 17:46:02 WARN DAGScheduler: Broadcasting large task binary with size 1306.9 KiB\n",
      "24/03/30 17:46:02 WARN DAGScheduler: Broadcasting large task binary with size 1385.0 KiB\n",
      "24/03/30 17:46:02 WARN DAGScheduler: Broadcasting large task binary with size 1434.0 KiB\n",
      "24/03/30 17:46:04 WARN DAGScheduler: Broadcasting large task binary with size 1078.4 KiB\n",
      "24/03/30 17:46:04 WARN DAGScheduler: Broadcasting large task binary with size 1350.7 KiB\n",
      "24/03/30 17:46:06 WARN DAGScheduler: Broadcasting large task binary with size 1078.4 KiB\n",
      "24/03/30 17:46:06 WARN DAGScheduler: Broadcasting large task binary with size 1350.7 KiB\n",
      "24/03/30 17:46:06 WARN DAGScheduler: Broadcasting large task binary with size 1590.9 KiB\n",
      "24/03/30 17:46:06 WARN DAGScheduler: Broadcasting large task binary with size 1784.9 KiB\n",
      "24/03/30 17:46:06 WARN DAGScheduler: Broadcasting large task binary with size 1932.1 KiB\n",
      "24/03/30 17:46:06 WARN DAGScheduler: Broadcasting large task binary with size 2031.2 KiB\n",
      "24/03/30 17:46:06 WARN DAGScheduler: Broadcasting large task binary with size 2033.1 KiB\n",
      "24/03/30 17:46:12 WARN DAGScheduler: Broadcasting large task binary with size 1079.6 KiB\n",
      "24/03/30 17:46:12 WARN DAGScheduler: Broadcasting large task binary with size 1209.8 KiB\n",
      "24/03/30 17:46:12 WARN DAGScheduler: Broadcasting large task binary with size 1309.3 KiB\n",
      "24/03/30 17:46:12 WARN DAGScheduler: Broadcasting large task binary with size 1381.1 KiB\n",
      "24/03/30 17:46:12 WARN DAGScheduler: Broadcasting large task binary with size 1426.6 KiB\n",
      "24/03/30 17:46:14 WARN DAGScheduler: Broadcasting large task binary with size 1083.5 KiB\n",
      "24/03/30 17:46:14 WARN DAGScheduler: Broadcasting large task binary with size 1357.3 KiB\n",
      "24/03/30 17:46:15 WARN DAGScheduler: Broadcasting large task binary with size 1083.5 KiB\n",
      "24/03/30 17:46:15 WARN DAGScheduler: Broadcasting large task binary with size 1357.3 KiB\n",
      "24/03/30 17:46:15 WARN DAGScheduler: Broadcasting large task binary with size 1597.8 KiB\n",
      "24/03/30 17:46:16 WARN DAGScheduler: Broadcasting large task binary with size 1789.0 KiB\n",
      "24/03/30 17:46:16 WARN DAGScheduler: Broadcasting large task binary with size 1927.4 KiB\n",
      "24/03/30 17:46:16 WARN DAGScheduler: Broadcasting large task binary with size 2019.8 KiB\n",
      "24/03/30 17:46:16 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/03/30 17:46:22 WARN DAGScheduler: Broadcasting large task binary with size 1096.1 KiB\n",
      "24/03/30 17:46:22 WARN DAGScheduler: Broadcasting large task binary with size 1229.9 KiB\n",
      "24/03/30 17:46:22 WARN DAGScheduler: Broadcasting large task binary with size 1330.1 KiB\n",
      "24/03/30 17:46:22 WARN DAGScheduler: Broadcasting large task binary with size 1402.1 KiB\n",
      "24/03/30 17:46:22 WARN DAGScheduler: Broadcasting large task binary with size 1450.6 KiB\n",
      "24/03/30 17:46:23 WARN DAGScheduler: Broadcasting large task binary with size 1084.3 KiB\n",
      "24/03/30 17:46:24 WARN DAGScheduler: Broadcasting large task binary with size 1355.0 KiB\n",
      "24/03/30 17:46:25 WARN DAGScheduler: Broadcasting large task binary with size 1084.3 KiB\n",
      "24/03/30 17:46:25 WARN DAGScheduler: Broadcasting large task binary with size 1355.0 KiB\n",
      "24/03/30 17:46:25 WARN DAGScheduler: Broadcasting large task binary with size 1591.7 KiB\n",
      "24/03/30 17:46:25 WARN DAGScheduler: Broadcasting large task binary with size 1780.0 KiB\n",
      "24/03/30 17:46:25 WARN DAGScheduler: Broadcasting large task binary with size 1922.4 KiB\n",
      "24/03/30 17:46:25 WARN DAGScheduler: Broadcasting large task binary with size 2023.3 KiB\n",
      "24/03/30 17:46:26 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/03/30 17:46:31 WARN DAGScheduler: Broadcasting large task binary with size 1065.7 KiB\n",
      "24/03/30 17:46:32 WARN DAGScheduler: Broadcasting large task binary with size 1205.6 KiB\n",
      "24/03/30 17:46:32 WARN DAGScheduler: Broadcasting large task binary with size 1313.9 KiB\n",
      "24/03/30 17:46:32 WARN DAGScheduler: Broadcasting large task binary with size 1391.3 KiB\n",
      "24/03/30 17:46:32 WARN DAGScheduler: Broadcasting large task binary with size 1377.0 KiB\n",
      "24/03/30 17:46:34 WARN DAGScheduler: Broadcasting large task binary with size 1058.1 KiB\n",
      "24/03/30 17:46:34 WARN DAGScheduler: Broadcasting large task binary with size 1325.4 KiB\n",
      "24/03/30 17:46:35 WARN DAGScheduler: Broadcasting large task binary with size 1058.1 KiB\n",
      "24/03/30 17:46:35 WARN DAGScheduler: Broadcasting large task binary with size 1325.4 KiB\n",
      "24/03/30 17:46:35 WARN DAGScheduler: Broadcasting large task binary with size 1565.3 KiB\n",
      "24/03/30 17:46:36 WARN DAGScheduler: Broadcasting large task binary with size 1762.7 KiB\n",
      "24/03/30 17:46:36 WARN DAGScheduler: Broadcasting large task binary with size 1916.1 KiB\n",
      "24/03/30 17:46:36 WARN DAGScheduler: Broadcasting large task binary with size 2021.2 KiB\n",
      "24/03/30 17:46:36 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/03/30 17:46:41 WARN DAGScheduler: Broadcasting large task binary with size 1086.2 KiB\n",
      "24/03/30 17:46:41 WARN DAGScheduler: Broadcasting large task binary with size 1217.9 KiB\n",
      "24/03/30 17:46:41 WARN DAGScheduler: Broadcasting large task binary with size 1316.5 KiB\n",
      "24/03/30 17:46:41 WARN DAGScheduler: Broadcasting large task binary with size 1390.1 KiB\n",
      "24/03/30 17:46:41 WARN DAGScheduler: Broadcasting large task binary with size 1370.5 KiB\n",
      "24/03/30 17:46:43 WARN DAGScheduler: Broadcasting large task binary with size 1075.7 KiB\n",
      "24/03/30 17:46:43 WARN DAGScheduler: Broadcasting large task binary with size 1348.8 KiB\n",
      "24/03/30 17:46:44 WARN DAGScheduler: Broadcasting large task binary with size 1075.7 KiB\n",
      "24/03/30 17:46:45 WARN DAGScheduler: Broadcasting large task binary with size 1348.8 KiB\n",
      "24/03/30 17:46:45 WARN DAGScheduler: Broadcasting large task binary with size 1584.2 KiB\n",
      "24/03/30 17:46:45 WARN DAGScheduler: Broadcasting large task binary with size 1779.1 KiB\n",
      "24/03/30 17:46:45 WARN DAGScheduler: Broadcasting large task binary with size 1935.2 KiB\n",
      "24/03/30 17:46:45 WARN DAGScheduler: Broadcasting large task binary with size 2047.0 KiB\n",
      "24/03/30 17:46:45 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/03/30 17:46:51 WARN DAGScheduler: Broadcasting large task binary with size 1061.0 KiB\n",
      "24/03/30 17:46:51 WARN DAGScheduler: Broadcasting large task binary with size 1192.0 KiB\n",
      "24/03/30 17:46:51 WARN DAGScheduler: Broadcasting large task binary with size 1291.3 KiB\n",
      "24/03/30 17:46:51 WARN DAGScheduler: Broadcasting large task binary with size 1362.3 KiB\n",
      "24/03/30 17:46:51 WARN DAGScheduler: Broadcasting large task binary with size 1411.8 KiB\n",
      "24/03/30 17:46:53 WARN DAGScheduler: Broadcasting large task binary with size 1064.3 KiB\n",
      "24/03/30 17:46:53 WARN DAGScheduler: Broadcasting large task binary with size 1328.7 KiB\n",
      "24/03/30 17:46:54 WARN DAGScheduler: Broadcasting large task binary with size 1064.3 KiB\n",
      "24/03/30 17:46:54 WARN DAGScheduler: Broadcasting large task binary with size 1328.7 KiB\n",
      "24/03/30 17:46:54 WARN DAGScheduler: Broadcasting large task binary with size 1553.4 KiB\n",
      "24/03/30 17:46:54 WARN DAGScheduler: Broadcasting large task binary with size 1737.5 KiB\n",
      "24/03/30 17:46:55 WARN DAGScheduler: Broadcasting large task binary with size 1885.8 KiB\n",
      "24/03/30 17:46:55 WARN DAGScheduler: Broadcasting large task binary with size 2002.4 KiB\n",
      "24/03/30 17:46:55 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/03/30 17:47:01 WARN DAGScheduler: Broadcasting large task binary with size 1094.6 KiB\n",
      "24/03/30 17:47:01 WARN DAGScheduler: Broadcasting large task binary with size 1230.8 KiB\n",
      "24/03/30 17:47:01 WARN DAGScheduler: Broadcasting large task binary with size 1333.5 KiB\n",
      "24/03/30 17:47:01 WARN DAGScheduler: Broadcasting large task binary with size 1405.6 KiB\n",
      "24/03/30 17:47:01 WARN DAGScheduler: Broadcasting large task binary with size 1454.0 KiB\n",
      "24/03/30 17:47:03 WARN DAGScheduler: Broadcasting large task binary with size 1094.0 KiB\n",
      "24/03/30 17:47:03 WARN DAGScheduler: Broadcasting large task binary with size 1365.2 KiB\n",
      "24/03/30 17:47:04 WARN DAGScheduler: Broadcasting large task binary with size 1094.0 KiB\n",
      "24/03/30 17:47:04 WARN DAGScheduler: Broadcasting large task binary with size 1365.2 KiB\n",
      "24/03/30 17:47:04 WARN DAGScheduler: Broadcasting large task binary with size 1602.2 KiB\n",
      "24/03/30 17:47:04 WARN DAGScheduler: Broadcasting large task binary with size 1791.3 KiB\n",
      "24/03/30 17:47:05 WARN DAGScheduler: Broadcasting large task binary with size 1935.1 KiB\n",
      "24/03/30 17:47:05 WARN DAGScheduler: Broadcasting large task binary with size 2035.8 KiB\n",
      "24/03/30 17:47:05 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/03/30 17:47:10 WARN DAGScheduler: Broadcasting large task binary with size 1079.3 KiB\n",
      "24/03/30 17:47:10 WARN DAGScheduler: Broadcasting large task binary with size 1213.3 KiB\n",
      "24/03/30 17:47:10 WARN DAGScheduler: Broadcasting large task binary with size 1317.2 KiB\n",
      "24/03/30 17:47:10 WARN DAGScheduler: Broadcasting large task binary with size 1388.5 KiB\n",
      "24/03/30 17:47:10 WARN DAGScheduler: Broadcasting large task binary with size 1369.3 KiB\n",
      "24/03/30 17:47:12 WARN DAGScheduler: Broadcasting large task binary with size 1083.0 KiB\n",
      "24/03/30 17:47:12 WARN DAGScheduler: Broadcasting large task binary with size 1354.9 KiB\n",
      "24/03/30 17:47:13 WARN DAGScheduler: Broadcasting large task binary with size 1083.0 KiB\n",
      "24/03/30 17:47:13 WARN DAGScheduler: Broadcasting large task binary with size 1354.9 KiB\n",
      "24/03/30 17:47:13 WARN DAGScheduler: Broadcasting large task binary with size 1597.4 KiB\n",
      "24/03/30 17:47:13 WARN DAGScheduler: Broadcasting large task binary with size 1800.6 KiB\n",
      "24/03/30 17:47:14 WARN DAGScheduler: Broadcasting large task binary with size 1960.6 KiB\n",
      "24/03/30 17:47:14 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/03/30 17:47:14 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/03/30 17:47:20 WARN DAGScheduler: Broadcasting large task binary with size 1109.0 KiB\n",
      "24/03/30 17:47:20 WARN DAGScheduler: Broadcasting large task binary with size 1252.5 KiB\n",
      "24/03/30 17:47:20 WARN DAGScheduler: Broadcasting large task binary with size 1361.0 KiB\n",
      "24/03/30 17:47:20 WARN DAGScheduler: Broadcasting large task binary with size 1430.2 KiB\n",
      "24/03/30 17:47:20 WARN DAGScheduler: Broadcasting large task binary with size 1402.8 KiB\n",
      "24/03/30 17:47:22 WARN DAGScheduler: Broadcasting large task binary with size 1064.4 KiB\n",
      "24/03/30 17:47:22 WARN DAGScheduler: Broadcasting large task binary with size 1329.1 KiB\n",
      "24/03/30 17:47:23 WARN DAGScheduler: Broadcasting large task binary with size 1064.4 KiB\n",
      "24/03/30 17:47:23 WARN DAGScheduler: Broadcasting large task binary with size 1329.1 KiB\n",
      "24/03/30 17:47:23 WARN DAGScheduler: Broadcasting large task binary with size 1566.3 KiB\n",
      "24/03/30 17:47:23 WARN DAGScheduler: Broadcasting large task binary with size 1763.7 KiB\n",
      "24/03/30 17:47:23 WARN DAGScheduler: Broadcasting large task binary with size 1912.3 KiB\n",
      "24/03/30 17:47:24 WARN DAGScheduler: Broadcasting large task binary with size 2012.8 KiB\n",
      "24/03/30 17:47:24 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Define a parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(model.numTrees, [10, 20,30]) \\\n",
    "    .addGrid(model.maxDepth, [5, 10,15]) \\\n",
    "    .build()\n",
    "\n",
    "# Configure CrossValidator\n",
    "crossval = CrossValidator(estimator=pipeline2,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(labelCol=\"quality\", predictionCol=\"prediction\", metricName=\"f1\"),\n",
    "                            # evaluator=RegressionEvaluator(labelCol=\"quality\"),\n",
    "                          numFolds=15)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model F1 Score on Test Data: 0.5853148496240602\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test data\n",
    "predictions = cvModel.transform(test_df)\n",
    "predictions = predictions.withColumn(\"prediction\", round(col(\"prediction\"),0).cast(\"double\"))\n",
    "# Evaluate the best model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"quality\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_score = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Best Model F1 Score on Test Data: {f1_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------+--------------------+\n",
      "|       prediction|quality|  normalizedFeatures|\n",
      "+-----------------+-------+--------------------+\n",
      "|5.268244298656669|      5|[0.25454545454545...|\n",
      "|4.927972027972028|      5|[0.29090909090909...|\n",
      "|5.009744641323589|      5|[0.29090909090909...|\n",
      "|5.195962732919254|      6|[0.6,0.1095890410...|\n",
      "|5.268244298656669|      5|[0.25454545454545...|\n",
      "+-----------------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select example rows to display\n",
    "predictions.select(\"prediction\", 'quality', \"normalizedFeatures\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Data source must be a DataFrame or Mapping, not <class 'pyspark.sql.dataframe.DataFrame'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcatplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/ml-winequality/.venv/lib/python3.10/site-packages/seaborn/categorical.py:2782\u001b[0m, in \u001b[0;36mcatplot\u001b[0;34m(data, x, y, hue, row, col, kind, estimator, errorbar, n_boot, seed, units, weights, order, hue_order, row_order, col_order, col_wrap, height, aspect, log_scale, native_scale, formatter, orient, color, palette, hue_norm, legend, legend_out, sharex, sharey, margin_titles, facet_kws, ci, **kwargs)\u001b[0m\n\u001b[1;32m   2779\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2780\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot pass values for both `x` and `y`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2782\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43mPlotter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2785\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\n\u001b[1;32m   2786\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2787\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2788\u001b[0m \u001b[43m    \u001b[49m\u001b[43morient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2789\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Handle special backwards compatibility where pointplot originally\u001b[39;49;00m\n\u001b[1;32m   2790\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# did *not* default to multi-colored unless a palette was specified.\u001b[39;49;00m\n\u001b[1;32m   2791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpoint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpalette\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlegend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlegend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2793\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2795\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   2796\u001b[0m     \u001b[38;5;66;03m# Handle faceting variables that lack name information\u001b[39;00m\n\u001b[1;32m   2797\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m p\u001b[38;5;241m.\u001b[39mvariables \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mvariables[var] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/workspaces/ml-winequality/.venv/lib/python3.10/site-packages/seaborn/categorical.py:67\u001b[0m, in \u001b[0;36m_CategoricalPlotter.__init__\u001b[0;34m(self, data, variables, order, orient, require_numeric, color, legend)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     58\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     legend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     65\u001b[0m ):\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# This method takes care of some bookkeeping that is necessary because the\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# original categorical plots (prior to the 2021 refactor) had some rules that\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# don't fit exactly into VectorPlotter logic. It may be wise to have a second\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# default VectorPlotter rules. If we do decide to make orient part of the\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m# _base variable assignment, we'll want to figure out how to express that.\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwide\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m orient \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/workspaces/ml-winequality/.venv/lib/python3.10/site-packages/seaborn/_base.py:634\u001b[0m, in \u001b[0;36mVectorPlotter.__init__\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;66;03m# var_ordered is relevant only for categorical axis variables, and may\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;66;03m# be better handled by an internal axis information object that tracks\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# such information and is set up by the scale_* methods. The analogous\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# information for numeric axes would be information about log scales.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_ordered \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}  \u001b[38;5;66;03m# alt., used DefaultDict\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;66;03m# TODO Lots of tests assume that these are called to initialize the\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;66;03m# mappings to default values on class initialization. I'd prefer to\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;66;03m# move away from that and only have a mapping when explicitly called.\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyle\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/workspaces/ml-winequality/.venv/lib/python3.10/site-packages/seaborn/_base.py:679\u001b[0m, in \u001b[0;36mVectorPlotter.assign_variables\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;66;03m# When dealing with long-form input, use the newer PlotData\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;66;03m# object (internal but introduced for the objects interface)\u001b[39;00m\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;66;03m# to centralize / standardize data consumption logic.\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlong\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 679\u001b[0m     plot_data \u001b[38;5;241m=\u001b[39m \u001b[43mPlotData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    680\u001b[0m     frame \u001b[38;5;241m=\u001b[39m plot_data\u001b[38;5;241m.\u001b[39mframe\n\u001b[1;32m    681\u001b[0m     names \u001b[38;5;241m=\u001b[39m plot_data\u001b[38;5;241m.\u001b[39mnames\n",
      "File \u001b[0;32m/workspaces/ml-winequality/.venv/lib/python3.10/site-packages/seaborn/_core/data.py:57\u001b[0m, in \u001b[0;36mPlotData.__init__\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     53\u001b[0m     data: DataSource,\n\u001b[1;32m     54\u001b[0m     variables: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, VariableSpec],\n\u001b[1;32m     55\u001b[0m ):\n\u001b[0;32m---> 57\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mhandle_data_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     frame, names, ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_variables(data, variables)\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe \u001b[38;5;241m=\u001b[39m frame\n",
      "File \u001b[0;32m/workspaces/ml-winequality/.venv/lib/python3.10/site-packages/seaborn/_core/data.py:278\u001b[0m, in \u001b[0;36mhandle_data_source\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Mapping):\n\u001b[1;32m    277\u001b[0m     err \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData source must be a DataFrame or Mapping, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(data)\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(err)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[0;31mTypeError\u001b[0m: Data source must be a DataFrame or Mapping, not <class 'pyspark.sql.dataframe.DataFrame'>."
     ]
    }
   ],
   "source": [
    "# EDA \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "sns.catplot(x=\"prediction\",data=predictions,kind='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/06 19:46:42 WARN Utils: Your hostname, codespaces-233249 resolves to a loopback address: 127.0.0.1; using 172.16.5.4 instead (on interface eth0)\n",
      "24/04/06 19:46:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/06 19:46:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/06 19:47:03 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"TrainTestWorkflow\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalizedFeatures\n"
     ]
    }
   ],
   "source": [
    "   # rfModel = RandomForestRegressor.load(args.model_path)\n",
    "rfModel = RandomForestRegressor.load(\"/workspaces/ml-winequality/model/wine-prediction-RF\")\n",
    "print(rfModel.getFeaturesCol())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assembledFeatures\n",
      "normalizedFeatures\n"
     ]
    }
   ],
   "source": [
    "    # Load the saved MinMaxScaler model and apply it to the assembled features\n",
    "scalerModel = MinMaxScaler.load(\"/workspaces/ml-winequality/model/scalar-RF\")\n",
    "print(scalerModel.getInputCol())\n",
    "print(scalerModel.getOutputCol())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
